{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlfredoLobosC/03MIAR_algoritmos_de_optimizacion/blob/main/mates_ejercicios_evaluables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRABAJO GRUPAL - EJERCICIOS ENTREGABLES**"
      ],
      "metadata": {
        "id": "YPdg3c4FiQCz"
      },
      "id": "YPdg3c4FiQCz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrantes:\n",
        "- Alfredo Lobos Collao\n",
        "- Bryan Alava Calderon\n",
        "- Kenny Pizarro Luzón\n",
        "- Abner Chica Herrera"
      ],
      "metadata": {
        "id": "SfCvvRJ-D1KN"
      },
      "id": "SfCvvRJ-D1KN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2bee266",
      "metadata": {
        "id": "c2bee266"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import timeit\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7a574b-2b1b-44c0-8c65-5df58913fe25",
      "metadata": {
        "id": "af7a574b-2b1b-44c0-8c65-5df58913fe25"
      },
      "source": [
        "### **1a.** Implementa una función, determinante_recursivo, que obtenga el determinante de una matriz cuadrada utilizando la definicion recursiva de Laplace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca09b64-2bf6-4024-9ac1-f81115c07863",
      "metadata": {
        "id": "4ca09b64-2bf6-4024-9ac1-f81115c07863",
        "outputId": "7d768ae4-a05f-43cf-ff40-784f763b462e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz B:\n",
            "[[ 1.  2.  0. -1.]\n",
            " [ 3.  1.  2.  4.]\n",
            " [ 0. -2.  1.  3.]\n",
            " [ 2.  0.  1.  1.]]\n",
            "Determinante (Laplace): 2.0\n",
            "Tiempo Laplace: 0.000334 segundos\n"
          ]
        }
      ],
      "source": [
        "def determinante_laplace(A):\n",
        "    \"\"\"\n",
        "    Calcula el determinante de una matriz cuadrada A utilizando\n",
        "    la expansión de Laplace de forma recursiva.\n",
        "    \"\"\"\n",
        "    A = np.array(A, dtype=float)\n",
        "    n, m = A.shape\n",
        "\n",
        "    if n != m:\n",
        "        raise ValueError(\"La matriz debe ser cuadrada (n x n).\")\n",
        "\n",
        "    # Caso base\n",
        "    if n == 1:\n",
        "        return A[0,0]\n",
        "\n",
        "    if n == 2:\n",
        "        # Fórmula directa para 2x2: a*d - b*c\n",
        "        return A[0,0]*A[1,1] - A[0,1]*A[1,0]\n",
        "\n",
        "    det = 0.0\n",
        "    signo = 1\n",
        "\n",
        "    # Expandir por la primera fila (indice=0)\n",
        "    for j in range(n):\n",
        "      # Si el elemento es cero, el término no aporta al determinante\n",
        "      if A[0, j] != 0:\n",
        "        sub = np.delete(np.delete(A, 0, axis=0), j, axis=1)\n",
        "\n",
        "        #Aplicamos la fórmula de Laplace\n",
        "        det += signo * A[0, j] * determinante_laplace(sub)\n",
        "\n",
        "      #Cambiamos el signo para la siguiente columna\n",
        "      signo = -signo\n",
        "\n",
        "    return det\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Ejemplo: matriz 4x4\n",
        "# ------------------------------\n",
        "B = np.array([\n",
        "    [1, 2, 0, -1],\n",
        "    [3, 1, 2,  4],\n",
        "    [0,-2, 1,  3],\n",
        "    [2, 0, 1,  1]\n",
        "], dtype=float)\n",
        "\n",
        "print(\"Matriz B:\")\n",
        "print(B)\n",
        "\n",
        "# Tiempo con Laplace recursivo\n",
        "start = time.time()\n",
        "det_B_laplace = determinante_laplace(B)\n",
        "end = time.time()\n",
        "print(f\"Determinante (Laplace): {det_B_laplace}\")\n",
        "print(f\"Tiempo Laplace: {end - start:.6f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a785d74b-a320-47de-a925-8066c82de697",
      "metadata": {
        "id": "a785d74b-a320-47de-a925-8066c82de697"
      },
      "source": [
        "**-------------------------------------------------------------------------------------------------**\n",
        "\n",
        "### **1b.** Si A es una matriz cuadrada n×n y triangular (superior o inferior, es decir, con entradas nulas por debajo o por encima de la diagonal, respectivamente), ¿existe alguna forma de calcular de forma directa y sencilla su determinante? Justif́ıquese la respuesta."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86c9b2c-0a97-443b-bb53-950d9e842476",
      "metadata": {
        "id": "d86c9b2c-0a97-443b-bb53-950d9e842476"
      },
      "source": [
        "Sí, existe una forma **directa y sencilla** de calcular el determinante de una matriz cuadrada $A$ de $n \\times n$ que sea triangular (superior o inferior).\n",
        "\n",
        "Esta simplicidad surge de una propiedad fundamental en el álgebra lineal: el determinante de una matriz triangular es igual al **producto de sus componentes en la diagonal principal**,.\n",
        "\n",
        "### Justificación\n",
        "\n",
        "La justificación de esta regla se basa en la definición del determinante mediante la expansión por cofactores (o Laplace):\n",
        "\n",
        "1.  **Regla de Cálculo:** Si $A$ es una matriz triangular superior o inferior, su determinante se calcula multiplicando todos los elementos de su diagonal principal: $\\text{det} A = a_{11}a_{22}a_{33} \\ldots a_{nn}$.\n",
        "2.  **Costo Computacional:** Esta operación es la más sencilla posible, ya que requiere únicamente $n$ multiplicaciones, lo que la hace extremadamente eficiente y contrasta fuertemente con la alta complejidad de otros métodos, como la definición recursiva de Laplace, que crece factorialmente ($n!$).\n",
        "3.  **Fundamento Teórico:** Al calcular el determinante de una matriz triangular (por ejemplo, triangular superior) expandiendo por la primera fila:\n",
        "    *   $\\text{det} A = a_{11} C_{11} - a_{12} C_{12} + a_{13} C_{13} - \\ldots$\n",
        "    *   Como la matriz es triangular, todos los elementos $a_{1j}$ para $j > 1$ son cero.\n",
        "    *   Esto significa que el determinante se reduce simplemente a $a_{11}$ multiplicado por su cofactor $C_{11}$.\n",
        "    *   El menor resultante, $M_{11}$, es también una matriz triangular, y este proceso continúa de manera recursiva hasta que el determinante se convierte únicamente en el producto de los elementos diagonales.\n",
        "\n",
        "Esta propiedad hace que la **Eliminación Gaussiana** sea un método tan eficiente para el cálculo de determinantes: al reducir la matriz a una forma triangular superior (escalonada por renglones), el determinante de la matriz original se obtiene fácilmente a partir del producto de los elementos en la diagonal de la matriz triangular resultante.\n",
        "\n",
        "A continuación, se presenta un ejemplo de cálculo para una matriz triangular superior $A$. :\n",
        "\n",
        "Sea la matriz $A$ de $3 \\times 3$:\n",
        "\n",
        "$$ A = \\begin{pmatrix} 4 & 1 & 5 \\\\ 0 & -2 & 3 \\\\ 0 & 0 & 7 \\end{pmatrix} $$\n",
        "\n",
        "Dado que $A$ es una matriz triangular superior, su determinante $(\\text{det } A)$ se calcula multiplicando las entradas de su diagonal principal:\n",
        "\n",
        "$$ \\text{det}(A) = 4 \\times (-2) \\times 7 $$\n",
        "\n",
        "$$ \\text{det}(A) = -56 $$\n",
        "\n",
        "De manera similar, para una matriz triangular inferior $B$:\n",
        "\n",
        "$$ B = \\begin{pmatrix} 3 & 0 & 0 \\\\ 6 & 1 & 0 \\\\ -1 & 5 & 2 \\end{pmatrix} $$\n",
        "\n",
        "El determinante es:\n",
        "\n",
        "$$ \\text{det}(B) = 3 \\times 1 \\times 2 $$\n",
        "\n",
        "$$ \\text{det}(B) = 6 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb9c88e-5ca1-4f78-be7d-57d304013dc6",
      "metadata": {
        "id": "9fb9c88e-5ca1-4f78-be7d-57d304013dc6"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "\n",
        "### **1c.** Determínese de forma justificada cómo alteran el determinante de una matriz n×n las dos operaciones elementales siguientes:\n",
        "Intercambiar una fila (o columna) por otra fila (o columna).\n",
        "Sumar a una fila (o columna) otra fila (o columna) multiplicada por un escalar α."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f481925-6fb4-4fe8-a9e6-adade4d3d920",
      "metadata": {
        "id": "8f481925-6fb4-4fe8-a9e6-adade4d3d920"
      },
      "source": [
        "El impacto de las dos operaciones elementales mencionadas sobre el determinante de una matriz $n \\times n$ es determinante para métodos eficientes de cálculo, como la Eliminación Gaussiana, ya que permiten manipular la matriz sin alterar drásticamente el valor final del determinante.\n",
        "\n",
        "A continuación se detalla cómo cada operación altera el determinante:\n",
        "\n",
        "#### a. Intercambiar una fila (o columna) por otra fila (o columna)\n",
        "\n",
        "Esta operación altera el signo del determinante:\n",
        "\n",
        "*   **Efecto:** El intercambio de cualesquiera dos renglones (o columnas) distintos de la matriz $A$ tiene el efecto de **multiplicar el determinante de $A$ por $-1$**.\n",
        "\n",
        "*   **Justificación:** Si se denota la nueva matriz resultante como $B$, se cumple que $\\det(B) = -\\det(A)$. Esta propiedad asegura que la magnitud del determinante se conserva, aunque su orientación o signo cambie.\n",
        "\n",
        "#### b. Sumar a una fila (o columna) otra fila (o columna) multiplicada por un escalar $\\alpha$\n",
        "\n",
        "Esta es la operación más útil para la simplificación matricial, ya que **el determinante permanece inalterado**:\n",
        "\n",
        "*   **Efecto:** Si se suma un múltiplo escalar de un renglón (o columna) de $A$ a otro renglón (o columna) de $A$, **el determinante no cambia**.\n",
        "\n",
        "*   **Justificación:** Si se denota la nueva matriz resultante como $B$, se cumple que $\\det(B) = \\det(A)$. Esto se justifica porque si la matriz $A$ tiene dos renglones o columnas proporcionales (lo que sucedería temporalmente al aplicar la suma del múltiplo al renglón de destino, antes de simplificar), su determinante es cero. La suma del múltiplo de un renglón a otro se descompone de tal manera que incluye un determinante que es cero, dejando solo el determinante original.\n",
        "\n",
        "| Operación Elemental | Efecto sobre $\\det(A)$ |\n",
        "| :--- | :--- |\n",
        "| Intercambio de dos filas (o columnas) | Se **multiplica por $-1$**. |\n",
        "| Sumar un múltiplo escalar de una fila (o columna) a otra | El determinante **no se altera**. |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.d.** Método de eliminación de Gauss con pivoteo parcial\n"
      ],
      "metadata": {
        "id": "8O6Cacg4oRf1"
      },
      "id": "8O6Cacg4oRf1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La eliminación gaussiana es un algoritmo que transforma una matriz $A \\in \\mathbb{R}^{n \\times n}$ en una matriz triangular superior $U$ mediante la aplicación sistemática de operaciones elementales. El pivoteo parcial se incorpora para mejorar la estabilidad numérica del proceso; consiste en seleccionar, en cada etapa $k$, el elemento de mayor magnitud en la columna actual (desde la fila $k$ hasta la $n$) para intercambiarlo a la posición del pivote ($a_{kk}$).\n",
        "\n",
        "Esta técnica es crucial para evitar la división por cero o por valores extremadamente pequeños, lo que en sistemas computacionales amplificaría los errores de redondeo de punto flotante y degradaría la precisión de los resultados"
      ],
      "metadata": {
        "id": "KMjMSL7roc0D"
      },
      "id": "KMjMSL7roc0D"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def escalonar_matriz(A):\n",
        "    \"\"\"\n",
        "    Escalonamiento de matriz\n",
        "    \"\"\"\n",
        "    U = A.astype(float).copy()\n",
        "    n = U.shape[0]\n",
        "\n",
        "    for k in range(n):\n",
        "        # Selección del pivote (Pivoteo parcial)\n",
        "        idx_pivote = k + np.argmax(np.abs(U[k:, k]))\n",
        "\n",
        "        # Intercambio de filas (f_k <-> f_idx_pivote)\n",
        "        if idx_pivote != k:\n",
        "            U[[k, idx_pivote]] = U[[idx_pivote, k]]\n",
        "\n",
        "        # Proceso de eliminación hacia adelante\n",
        "        for i in range(k + 1, n):\n",
        "            if U[k, k] != 0:\n",
        "                factor = U[i, k] / U[k, k]\n",
        "                U[i, k:] -= factor * U[k, k:]\n",
        "\n",
        "    return U"
      ],
      "metadata": {
        "id": "MIbpYfVFoRGA"
      },
      "id": "MIbpYfVFoRGA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "824e457f-846b-46e6-b65f-647278bb7872",
      "metadata": {
        "id": "824e457f-846b-46e6-b65f-647278bb7872"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "\n",
        "### **1e**. ¿Cómo se podŕıa calcular el determinante de una matriz haciendo beneficio de la estrategia anterior y del efecto de aplicar las operaciones elementales pertinentes? Implementa una nueva función, determinante gauss, que calcule el determinante de una matriz utilizando eliminación gaussiana."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18ced5a-6162-455e-9c23-fed5e14d89dd",
      "metadata": {
        "id": "a18ced5a-6162-455e-9c23-fed5e14d89dd"
      },
      "source": [
        "Este código de Python implementa el método de **Eliminación Gaussiana con pivoteo parcial** y **sustitución hacia atrás** para resolver un sistema de ecuaciones lineales de la forma $A\\mathbf{x} = \\mathbf{b}$ (donde `A` es la matriz aumentada $[A|\\mathbf{b}]$).\n",
        "\n",
        "La Eliminación Gaussiana es el método estándar y eficiente para estos cálculos, con una complejidad de orden $O(n^3)$ para una matriz de $n \\times n$, lo que la hace mucho más rápida que los métodos recursivos (que tienen una complejidad de $O(n!)$). El algoritmo busca convertir la matriz en una **forma escalonada por renglones** (matriz triangular) utilizando operaciones elementales.\n",
        "\n",
        "El **pivoteo parcial** (cambio de filas para colocar el elemento más grande en la posición de pivote) se incorpora para mejorar la estabilidad numérica, como se sugiere en los métodos robustos de Álgebra Lineal [3099d, 2824].\n",
        "\n",
        "Este código le permite medir directamente el **coste computacional en tiempo** del algoritmo de Eliminación Gaussiana. En aplicaciones de Inteligencia Artificial y *machine learning*, donde las matrices suelen ser muy grandes, este método se prefiere debido a su eficiencia superior de orden $O(n^3)$, en contraste con la complejidad factorial ($O(n!)$) de la definición recursiva. Comparar este tiempo con el de otras implementaciones, como la función preprogramada `numpy.linalg.det` (si estuviera calculando el determinante) o una implementación recursiva, le ofrecerá una clara perspectiva práctica de las diferencias de eficiencia.\n",
        "\n",
        "Para incorporar el cálculo del determinante al código de Eliminación Gaussiana, debemos aprovechar el hecho de que, al finalizar la **Fase 1: Eliminación hacia Adelante**, la matriz de coeficientes $A$ se ha transformado en una matriz triangular superior ($U$).\n",
        "\n",
        "El determinante de una matriz triangular es simplemente el **producto de sus componentes en la diagonal principal** [3098, Conversational Context]. Sin embargo, dado que la implementación previa utilizaba **pivoteo parcial** (intercambio de filas), cada intercambio multiplica el determinante por $-1$. Por lo tanto, debemos contar el número de intercambios realizados y ajustar el signo del determinante de $U$ al final.\n",
        "\n",
        "A continuación, se presenta la función actualizada, que retorna el vector solución, el tiempo de ejecución y el determinante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ac87ab-8829-4f42-be20-1afbd2c38d6a",
      "metadata": {
        "id": "f1ac87ab-8829-4f42-be20-1afbd2c38d6a",
        "outputId": "3ef42e25-b47e-4778-e7eb-bb09ece1bebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determinante: -2.0000000000000004\n",
            "Tiempo de ejecución: 0.00051022 segundos\n"
          ]
        }
      ],
      "source": [
        "def eliminacion_gaussiana(A):\n",
        "  \"\"\"\n",
        "    Implementa la Eliminación Gaussiana con pivoteo parcial\n",
        "  \"\"\"\n",
        "  #Copiamos la matriz y la convertios a float para hacer divisiones\n",
        "  A = np.asarray(A, dtype = float).copy()\n",
        "\n",
        "  #Dimensión de la matriz (n x n)\n",
        "  n, m = A.shape\n",
        "\n",
        "  if n != m:\n",
        "        raise ValueError(\"El determinante solo está definido para matrices cuadradas.\")\n",
        "\n",
        "  #Contador intercambio de filas para cambiar el signo del determinante\n",
        "  num_swaps = 0\n",
        "\n",
        "  # === Fase 1: Eliminación hacia adelante (Reducción a forma escalonada U)\n",
        "  for k in range(n - 1):\n",
        "    #Buscamos el mayor absoluto en la columna K para el pivote\n",
        "    indice_pivote = k + np.argmax(np.abs(A[k:, k]))\n",
        "\n",
        "    # Si es singular, el determinante es cero.\n",
        "    if A[indice_pivote, k] == 0:\n",
        "          return 0.0\n",
        "\n",
        "    # Intercambio de filas si el pivote no está en la posición k\n",
        "    if indice_pivote != k:\n",
        "          A[[k, indice_pivote]] = A[[indice_pivote, k]]\n",
        "          num_swaps += 1\n",
        "\n",
        "    # 2. Eliminación de elementos debajo del pivote\n",
        "    #Se crean ceros en la columna k\n",
        "    factores = A[k+1:, k] / A[k, k]\n",
        "    A[k+1:, k:] -= factores[:, None] * A[k, k:]\n",
        "\n",
        "  # --- Cálculo del Determinante\n",
        "  #(det(A) = (-1)^swaps * det(U)) ---\n",
        "\n",
        "  det = np.prod(np.diag(A))\n",
        "  return (-1)**num_swaps * det\n",
        "\n",
        "# Ejemplo de uso:\n",
        "'''A_ejemplo = np.array([\n",
        "    [1., 2., -1., 9.],\n",
        "    [2., 5., -2., 14.],\n",
        "    [-3., 1., 1., 9.]\n",
        "])'''\n",
        "\n",
        "A_ejemplo = np.array([\n",
        "    [1., 2., -1.],\n",
        "    [2., 5., -2.],\n",
        "    [-3., 1., 1.]\n",
        "])\n",
        "\n",
        "inicio = time.time()\n",
        "det = eliminacion_gaussiana(A_ejemplo)\n",
        "fin = time.time()\n",
        "\n",
        "print(f\"Determinante: {det}\")\n",
        "print(f\"Tiempo de ejecución: {fin - inicio:.8f} segundos\")\n",
        "# El resultado obtenido para esta matriz de ejemplo es **$-2$**, lo cual concuerda con el cálculo que realizamos previamente utilizando la matriz triangular final [Conversational Context]. Este valor, al ser distinto de cero, confirma que la matriz es invertible y que el sistema tiene una solución única [3099, Conversational Context]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d6082e-0f7c-43a9-96f1-223a194a4b83",
      "metadata": {
        "id": "63d6082e-0f7c-43a9-96f1-223a194a4b83"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "### **1f**. Obtén la complejidad computacional asociada al cálculo del determinante con la definición recursiva y con el método de eliminación de Gauss con pivoteo parcial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db8487a-0830-46fa-b939-2c75d6a445de",
      "metadata": {
        "id": "2db8487a-0830-46fa-b939-2c75d6a445de"
      },
      "source": [
        "La comparación de la eficiencia y el coste computacional entre los diferentes métodos para el cálculo de determinantes es fundamental en el ámbito de la Inteligencia Artificial (IA) y el aprendizaje automático, dado que la cantidad de datos manejados suele ser muy grande, requiriendo algoritmos optimizados.\n",
        "\n",
        "Existen varias alternativas para calcular el determinante de una matriz $n \\times n$, siendo las más comunes la **definición recursiva (expansión por cofactores o Laplace)** y los métodos basados en la **eliminación de Gauss**.\n",
        "\n",
        "La principal diferencia en eficiencia radica en el **coste computacional** (o complejidad computacional) asociado a cada método, que mide la cantidad de operaciones aritméticas necesarias:\n",
        "\n",
        "#### a. Método de la Definición Recursiva (Laplace)\n",
        "\n",
        "Este método se basa en la expansión por cofactores (o recursiva de Laplace). Aunque es teóricamente directo, su coste computacional es extremadamente alto para matrices de gran tamaño.\n",
        "\n",
        "*   **Coste:** La complejidad de este método es tan laboriosa que el cálculo de un determinante $50 \\times 50$ mediante la definición recursiva se estima que tomaría alrededor de $4.8 \\times 10^{50}$ años con una computadora capaz de realizar $10^6$ operaciones por segundo. Esto se debe a que la complejidad crece con el factorial del tamaño de la matriz ($n!$).\n",
        "\n",
        "#### b. Método de Eliminación Gaussiana\n",
        "\n",
        "Este método aprovecha las propiedades de los determinantes para simplificar la matriz a una forma triangular, cuyo cálculo es trivial.\n",
        "\n",
        "*   **Estrategia:** Se utiliza la eliminación gaussiana (la cual puede incluir pivoteo parcial) para escalonar la matriz y convertirla en una matriz triangular.\n",
        "*   **Eficiencia:** Una vez que la matriz es triangular (superior o inferior), el determinante se calcula de forma directa y sencilla multiplicando las entradas de su diagonal.\n",
        "*   **Coste:** El coste computacional se reduce drásticamente porque la fase más pesada del proceso (la reducción a la forma triangular) tiene una complejidad de orden cúbico.\n",
        "    *   Para una matriz $n \\times n$, el número de multiplicaciones y sumas requeridas para resolver el sistema mediante la eliminación gaussiana (que incluye la sustitución hacia atrás) es de aproximadamente $\\frac{1}{3} n^3$.\n",
        "    *   En contraste con la definición recursiva, este coste de orden $O(n^3)$ es mucho más eficiente para grandes valores de $n$.\n",
        "\n",
        "### Comparación de eficiencia\n",
        "\n",
        "La **eliminación gaussiana** es el método preferido en la práctica para calcular determinantes debido a su eficiencia superior. La diferencia en el coste computacional es tal que, si bien la definición recursiva es útil para matrices muy pequeñas o para fines teóricos, el método de Gauss es esencial para manejar el gran volumen de datos que caracterizan las aplicaciones de IA.\n",
        "\n",
        "| Método de Cálculo | Estrategia | Coste Computacional (Para $n$ grande) | Eficiencia |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Recursivo (Laplace)** | Expansión por cofactores. | Relacionado con $n!$ | **Baja** (prohibitivamente alta para grandes $n$). |\n",
        "| **Gaussiana** | Reducción a forma triangular mediante operaciones elementales. | Aproximadamente $O(n^3)$ | **Alta** (método estándar para matrices grandes). |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c35a08c-c286-4859-8fc7-38f7fbca52c5",
      "metadata": {
        "id": "0c35a08c-c286-4859-8fc7-38f7fbca52c5",
        "outputId": "7ea75da1-7c54-4b53-c4be-509da0f70daa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determinante (NumPy): 2.000000000000001\n",
            "Tiempo NumPy: 0.000248 segundos\n"
          ]
        }
      ],
      "source": [
        "# Tiempo con NumPy para comparar con algoritmos anteriores\n",
        "start = time.time()\n",
        "det_B_numpy = np.linalg.det(B)\n",
        "end = time.time()\n",
        "print(f\"Determinante (NumPy): {det_B_numpy}\")\n",
        "print(f\"Tiempo NumPy: {end - start:.6f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab5fc7e4-8ca9-4386-b35a-bd3f2f7444e5",
      "metadata": {
        "id": "ab5fc7e4-8ca9-4386-b35a-bd3f2f7444e5"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "### **1g**. Utilizando numpy.random.rand, genera matrices cuadradas aleatorias de la forma An∈Rn×n,para 2 ≤n≤10, y confecciona una tabla comparativa del tiempo de ejecución asociado a cada una de las variantes siguientes, interpretando los resultados:\n",
        "- Utilizando determinante recursivo.\n",
        "- Empleando determinante gauss.\n",
        "- Haciendo uso de la función preprogramada numpy.linalg.det.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9babf7c-2628-4bb6-bb1b-623c68826140",
      "metadata": {
        "id": "c9babf7c-2628-4bb6-bb1b-623c68826140",
        "outputId": "59835620-ceb5-4a35-f509-d42d2c5edc69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    n  Determinante Laplace  Determinante Gauss  NumPy linalg.det\n",
              "0   2              0.000034            0.000213          0.000017\n",
              "1   3              0.000094            0.000158          0.000012\n",
              "2   4              0.000284            0.000189          0.000010\n",
              "3   5              0.001032            0.000211          0.000011\n",
              "4   6              0.005976            0.000256          0.000011\n",
              "5   7              0.054015            0.000156          0.000017\n",
              "6   8              0.467201            0.000256          0.000021\n",
              "7   9              4.994227            0.000183          0.000015\n",
              "8  10             32.718765            0.000200          0.000018"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53a895a7-b71d-468e-9ec4-4d37c6cb43df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>Determinante Laplace</th>\n",
              "      <th>Determinante Gauss</th>\n",
              "      <th>NumPy linalg.det</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>0.005976</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7</td>\n",
              "      <td>0.054015</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>0.467201</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9</td>\n",
              "      <td>4.994227</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10</td>\n",
              "      <td>32.718765</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53a895a7-b71d-468e-9ec4-4d37c6cb43df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-53a895a7-b71d-468e-9ec4-4d37c6cb43df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-53a895a7-b71d-468e-9ec4-4d37c6cb43df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_38d2c96e-74e5-4bd7-bd8a-9acd6fcf1fdd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tabla')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_38d2c96e-74e5-4bd7-bd8a-9acd6fcf1fdd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tabla');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tabla",
              "summary": "{\n  \"name\": \"tabla\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 10,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          9,\n          3,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Determinante Laplace\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.800403916789252,\n        \"min\": 3.372399999079789e-05,\n        \"max\": 32.71876535499999,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          4.9942268959999865,\n          9.378400000059628e-05,\n          0.054014579000011054\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Determinante Gauss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.62837965526849e-05,\n        \"min\": 0.00015563819999897532,\n        \"max\": 0.00025575880000019426,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.00018308839999576777,\n          0.0001582871999971758,\n          0.00015563819999897532\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NumPy linalg.det\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.642766630341842e-06,\n        \"min\": 1.0404600004676468e-05,\n        \"max\": 2.082660000155556e-05,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.4951399998608395e-05,\n          1.2411400001610673e-05,\n          1.709520000190423e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "### Programa Python: Comparación de Coste Computacional\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# DEFINICIÓN DE LA FUNCIÓN DE LAPLACE (RECURSIVA)\n",
        "# Fuente: Excerpts from \"determinante_laplace.txt\"-\n",
        "# Ajustada para auto-recursión.\n",
        "# ======================================================================\n",
        "\n",
        "def determinante_laplace(A):\n",
        "    \"\"\"\n",
        "    Calcula el determinante de una matriz cuadrada A utilizando\n",
        "    la expansión de Laplace de forma recursiva.\n",
        "    \"\"\"\n",
        "    A = np.array(A, dtype=float)\n",
        "    n, m = A.shape\n",
        "\n",
        "    if n != m:\n",
        "        raise ValueError(\"La matriz debe ser cuadrada (n x n).\")\n",
        "\n",
        "    # Caso base\n",
        "    if n == 1:\n",
        "        return A[0,0]\n",
        "\n",
        "    if n == 2:\n",
        "        # Fórmula directa para 2x2: a*d - b*c\n",
        "        return A[0,0]*A[1,1] - A[0,1]*A[1,0]\n",
        "\n",
        "    det = 0.0\n",
        "    signo = 1\n",
        "\n",
        "    # Expandir por la primera fila (indice=0)\n",
        "    for j in range(n):\n",
        "      # Si el elemento es cero, el término no aporta al determinante\n",
        "      if A[0, j] != 0:\n",
        "        sub = np.delete(np.delete(A, 0, axis=0), j, axis=1)\n",
        "\n",
        "        #Aplicamos la fórmula de Laplace\n",
        "        det += signo * A[0, j] * determinante_laplace(sub)\n",
        "\n",
        "      #Cambiamos el signo para la siguiente columna\n",
        "      signo = -signo\n",
        "\n",
        "    return det\n",
        "\n",
        "# ======================================================================\n",
        "# DEFINICIÓN DE LA FUNCIÓN GAUSSIANA (CON CÁLCULO DEL DETERMINANTE)\n",
        "# Fuente: Excerpts from \"determinante_gaussiana.txt\"-\n",
        "# ======================================================================\n",
        "\n",
        "def eliminacion_gaussiana_det(A):\n",
        "    \"\"\"\n",
        "    Implementa la Eliminación Gaussiana con pivoteo parcial\n",
        "    \"\"\"\n",
        "    #Copiamos la matriz y la convertios a float para hacer divisiones\n",
        "    A = np.asarray(A, dtype = float).copy()\n",
        "\n",
        "    #Dimensión de la matriz (n x n)\n",
        "    n, m = A.shape\n",
        "\n",
        "    if n != m:\n",
        "        raise ValueError(\"El determinante solo está definido para matrices cuadradas\")\n",
        "\n",
        "    #Contador intercambio de filas para cambiar el signo del determinante\n",
        "    num_swaps = 0\n",
        "\n",
        "    # === Fase 1: Eliminación hacia adelante (Reducción a forma escalonada U)\n",
        "    for k in range(n - 1):\n",
        "      #Buscamos el mayor absoluto en la columna K para el pivote\n",
        "      indice_pivote = k + np.argmax(np.abs(A[k:, k]))\n",
        "\n",
        "      # Si es singular, el determinante es cero.\n",
        "      if A[indice_pivote, k] == 0:\n",
        "            return 0.0\n",
        "\n",
        "      # Intercambio de filas si el pivote no está en la posición k\n",
        "      if indice_pivote != k:\n",
        "            A[[k, indice_pivote]] = A[[indice_pivote, k]]\n",
        "            num_swaps += 1\n",
        "\n",
        "      # 2. Eliminación de elementos debajo del pivote\n",
        "      #Se crean ceros en la columna k\n",
        "      factores = A[k+1:, k] / A[k, k]\n",
        "      A[k+1:, k:] -= factores[:, None] * A[k, k:]\n",
        "\n",
        "    # --- Cálculo del Determinante\n",
        "    #(det(A) = (-1)^swaps * det(U)) ---\n",
        "\n",
        "    det = np.prod(np.diag(A))\n",
        "    return (-1)**num_swaps * det\n",
        "\n",
        "# ======================================================================\n",
        "# EJECUCIÓN PRINCIPAL Y COMPARACIÓN\n",
        "# ======================================================================\n",
        "\n",
        "def ejecutar_comparacion():\n",
        "  resultados = []\n",
        "\n",
        "  for n in range(2, 11):\n",
        "      A = np.random.rand(n, n)\n",
        "\n",
        "      t_laplace = timeit.timeit(\n",
        "          lambda: determinante_laplace(A), #Ejecutamos la funcion para calcular la determinante método Laplace\n",
        "          number=1\n",
        "      )\n",
        "\n",
        "      t_gauss = timeit.timeit(\n",
        "          lambda: eliminacion_gaussiana_det(A), #Ejecutamos la funcion para calcular la determinante método Gaussiana\n",
        "          number=5 #Calculamos el tiempo promedio de 5 ejecuciones\n",
        "      ) / 5\n",
        "\n",
        "      t_numpy = timeit.timeit(\n",
        "          lambda: np.linalg.det(A), #Ejecutamos la funcion para calcular la determinante método Numpy\n",
        "          number=5\n",
        "      ) / 5 # Calculamos el tiempo promedio de 5 ejecuciones\n",
        "\n",
        "      resultados.append([n, t_laplace, t_gauss, t_numpy])\n",
        "\n",
        "  return pd.DataFrame(\n",
        "      resultados,\n",
        "      columns=[\n",
        "          \"n\",\n",
        "          \"Determinante Laplace\",\n",
        "          \"Determinante Gauss\",\n",
        "          \"NumPy linalg.det\"\n",
        "      ]\n",
        "  )\n",
        "\n",
        "\n",
        "tabla = ejecutar_comparacion()\n",
        "display(tabla)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Interpretación de los resultados**\n",
        "\n",
        "\n",
        "Al comparar los tiempos, lo primero que salta a la vista es lo rápido que se crece el tiempo de cálculo y ejecución para el método de Laplace. Mientras que para una matriz de $2\\times2$ apenas tarda 0.00003s, al llegar a una de $10\\times10$ el tiempo aumenta de golpe a más de 30 segundos. Esto confirma en la práctica lo que dice la teoría sobre la complejidad factorial $O(n!)$: cada vez que añadimos una fila, el esfuerzo de cálculo se multiplica de forma inmanejable incluso para matrices muy pequeñas.\n",
        "\n",
        "Por el contrario, la eliminación gaussiana resuelve el mismo determinante de $10\\times10$ en apenas 0.0003 segundos, validando que el uso de operaciones elementales para obtener una matriz triangular reduce drásticamente el coste a un orden cúbico ($O(n^3)$). Finalmente, la función de NumPy muestra el tiempo más bajo (0.00002s), lo que evidencia la efectividad de las implementaciones optimizadas para el cálculo matricial. Estos datos demuestran que, para cualquier análisis que requiera trabajar con matrices de dimensiones crecientes, el método recursivo es inviable frente a la eficiencia de la reducción gaussiana y las librerías especializadas"
      ],
      "metadata": {
        "id": "Z4Us_zO5UiWY"
      },
      "id": "Z4Us_zO5UiWY"
    },
    {
      "cell_type": "markdown",
      "id": "c0b146dc-297e-485a-98b1-1855f34ed5a0",
      "metadata": {
        "id": "c0b146dc-297e-485a-98b1-1855f34ed5a0"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "**2a.**  Prográmese en Python el método de descenso de gradiente para funciones de nvariables. La función deberá tener como parámetros de entradas:\n",
        "El gradiente de la función que se desea minimizar ∇f(puede venir dada como otra función previamente implementada, grad f, con entrada un vector, representando el punto donde se quiere calcular el gradiente, y salida otro vector, representando el gradiente de fen dicho punto).\n",
        "Un valor inicial x0∈Rn(almacenado en un vector de ncomponentes).\n",
        "El ratio de aprendizaje γ(que se asume constante para cada iteración).\n",
        "Un parámetro de tolerancia tol (con el que finalizar el proceso cuando ∥∇f(x)∥2<tol).\n",
        "Un número máximo de iteraciones maxit (con el fin de evitar ejecuciones indefinidas en caso de divergencia o convergencia muy lenta).\n",
        "La salida de la función deberá ser la aproximación del xque cumple f′(x)≈0, corres-pondiente a la última iteración realizada en el método."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce70d3d4",
      "metadata": {
        "id": "ce70d3d4"
      },
      "outputs": [],
      "source": [
        "def evaluar_funcion(f, x_vec):\n",
        "    \"\"\"\n",
        "    Intenta ejecutar la función f adaptándose a cómo la definió el usuario.\n",
        "    Soporta: f(x), f(x, y, z), f([x, y]).\n",
        "    \"\"\"\n",
        "    # 1. Si es 1D, intentamos pasarle el escalar directo\n",
        "    if len(x_vec) == 1:\n",
        "        try:\n",
        "            return f(x_vec[0]) # Intenta f(3.0)\n",
        "        except TypeError:\n",
        "            return f(x_vec)    # Si falla, intenta f([3.0])\n",
        "\n",
        "    # 2. Si es nD, probamos pasar el vector o desempaquetarlo\n",
        "    try:\n",
        "        return f(x_vec)       # Intenta f([x, y]) (Estilo vectorial)\n",
        "    except TypeError:\n",
        "        return f(*x_vec)      # Intenta f(x, y) (Estilo argumentos separados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec577eca",
      "metadata": {
        "id": "ec577eca"
      },
      "outputs": [],
      "source": [
        "def generar_gradiente(f, h=1e-5):\n",
        "    \"\"\"Genera la función de gradiente automáticamente.\"\"\"\n",
        "    def grad_f_numerico(x):\n",
        "        x = np.array(x, dtype=float)\n",
        "        n = len(x)\n",
        "        gradiente = np.zeros(n)\n",
        "\n",
        "        for i in range(n):\n",
        "            x_mas, x_menos = x.copy(), x.copy()\n",
        "            x_mas[i] += h\n",
        "            x_menos[i] -= h\n",
        "\n",
        "            # Usamos el evaluador inteligente aquí\n",
        "            y_mas = evaluar_funcion(f, x_mas)\n",
        "            y_menos = evaluar_funcion(f, x_menos)\n",
        "\n",
        "            gradiente[i] = (y_mas - y_menos) / (2 * h)\n",
        "        return gradiente\n",
        "    return grad_f_numerico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26f486d",
      "metadata": {
        "id": "b26f486d"
      },
      "outputs": [],
      "source": [
        "def resolver_optimizacion(funcion, x0, gamma, tol, maxit):\n",
        "\n",
        "    # Convertimos x0 a vector numpy, detecta la dimensión a usar\n",
        "    x_actual = np.atleast_1d(np.array(x0, dtype=float))\n",
        "    dim = len(x_actual)\n",
        "\n",
        "    print(f\"--> Detectado problema de {dim} variable(s).\")\n",
        "\n",
        "    # Generamos el gradiente\n",
        "    grad_f = generar_gradiente(funcion)\n",
        "\n",
        "    # Bucle de descenso\n",
        "    for k in range(int(maxit)):\n",
        "        grad = grad_f(x_actual)\n",
        "        norma = np.linalg.norm(grad)\n",
        "\n",
        "        if norma < tol:\n",
        "            print(f\"--> Convergencia en iteración {k}\")\n",
        "            break\n",
        "\n",
        "        x_actual = x_actual - gamma * grad\n",
        "    else:\n",
        "        print(\"--> Se alcanzó el máximo de iteraciones.\")\n",
        "\n",
        "    # Resultado final\n",
        "    y_final = evaluar_funcion(funcion, x_actual)\n",
        "    return x_actual, y_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8014e3d-90f8-47e8-9cdf-df164820e6d4",
      "metadata": {
        "id": "e8014e3d-90f8-47e8-9cdf-df164820e6d4"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "2bi. Sea la función f: R →R dada por f(x)= 3x4+ 4x3− 12x2+ 7.\n",
        "\n",
        "     Aplica el método sobre f(x)con x0= 3 γ= 0.001,tol=1e-12, maxit=1e5.\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(x):\n",
        "    return 3*x**4 + 4*x**3 - 12*x**2 + 7"
      ],
      "metadata": {
        "id": "lBDdng38PYuU"
      },
      "id": "lBDdng38PYuU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = 3.0  # Valor inicial x0 ∈ R^n (n=1)\n",
        "gamma = 0.001         # Ratio de aprendizaje (γ)\n",
        "tol = 1e-12           # Tolerancia (tol)\n",
        "maxit = 1e5\n",
        "x_res, y_res = resolver_optimizacion(f1,x0, gamma, tol, maxit)\n",
        "print(f\"Mínimo en x={x_res[0]:.4f}, f(x)={y_res:.4f}\\n\")"
      ],
      "metadata": {
        "id": "9KMewD_tN4w-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf800fe-6975-4649-b214-95dacf5339db"
      },
      "id": "9KMewD_tN4w-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Detectado problema de 1 variable(s).\n",
            "--> Convergencia en iteración 746\n",
            "Mínimo en x=1.0000, f(x)=2.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cdb00b2-17ce-4254-841a-e0d2b27533cb",
      "metadata": {
        "id": "5cdb00b2-17ce-4254-841a-e0d2b27533cb"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "2b ii Aplica de nuevo el método sobre f(x)con x0= 3, γ= 0.01,tol=1e-12, maxit=1e5.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = 3.0  # Valor inicial x0 ∈ R^n (n=1)\n",
        "gamma = 0.01         # Ratio de aprendizaje (γ)\n",
        "tol = 1e-12           # Tolerancia (tol)\n",
        "maxit = 1e5\n",
        "x_res, y_res = resolver_optimizacion(f1,x0, gamma, tol, maxit)\n",
        "print(f\"Mínimo en x={x_res[0]:.4f}, f(x)={y_res:.4f}\\n\")"
      ],
      "metadata": {
        "id": "RDsTPVFfOHRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd79d5f-931e-4d02-febb-f41b0b3181ca"
      },
      "id": "RDsTPVFfOHRN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Detectado problema de 1 variable(s).\n",
            "--> Convergencia en iteración 26\n",
            "Mínimo en x=-2.0000, f(x)=-25.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0348684f-5425-4dcd-b05d-23cc262db4a6",
      "metadata": {
        "id": "0348684f-5425-4dcd-b05d-23cc262db4a6"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "2b iii Contrasta e interpreta los dos resultados obtenidos en los apartados anteriores y compáralos con los ḿınimos locales obtenidos anaĺıticamente. ¿Qué influencia puede llegar a tener la elección del ratio de aprendizaje γ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef343d9-5fa6-47a2-83af-538f3bbabf6a",
      "metadata": {
        "id": "aef343d9-5fa6-47a2-83af-538f3bbabf6a"
      },
      "source": [
        "la **sensibilidad del método del Descenso de Gradiente al ratio de aprendizaje ($\\gamma$)**.\n",
        "\n",
        "**$x = -2$** es, junto con $x=0$ y $x=1$, uno de los **puntos críticos** de la función $f(x) = 3x^4 + 4x^3 - 12x^2 + 7$ (es decir, donde la derivada $\\nabla f(x)$ es cero)\n",
        "\n",
        "Al modificar el ratio de aprendizaje de $\\gamma = 0.001$ (que inicialmente lo llevaba a la convergencia cerca de $x \\approx 1$ o $x \\approx 1.0$ a $\\gamma = 0.01$, el algoritmo alteró significativamente su camino de optimización:\n",
        "\n",
        "** El valor de $\\gamma$ determina la magnitud del paso tomado en cada iteración en la dirección opuesta al gradiente.\n",
        "\n",
        "Al aumentar $\\gamma$ diez veces (de $0.001$ a $0.01$), el algoritmo comenzó a tomar pasos más grandes. Partiendo de $x_0 = 3$, este cambio fue suficiente para que la trayectoria de descenso **saltara** la cuenca de atracción del mínimo local más cercano a $x=1$ y en su lugar, se dirigiera hacia la cuenca de atracción del punto crítico en $x=-2$.\n",
        "\n",
        "Dado que $x=-2$ es un punto crítico, la convergencia ahí es una solución válida para el algoritmo de Descenso de Gradiente, que busca minimizar la función de coste encontrando un punto donde la norma del gradiente es menor que la tolerancia ($\\|\\nabla f(x)\\|_2 < \\text{tol}$).\n",
        "\n",
        "Este ejercicio confirma que la elección del ratio de aprendizaje es un hiperparámetro **crítico**:\n",
        "\n",
        "*   Si $\\gamma$ es demasiado **pequeño**, la convergencia puede ser muy lenta.\n",
        "*   Si $\\gamma$ es **adecuado** para la trayectoria, puede encontrar un mínimo local.\n",
        "*   Si $\\gamma$ es **más grande** (como $0.01$ en este caso), la trayectoria cambia, lo que puede llevar a la convergencia en un mínimo local completamente diferente (como $x = -2$).\n",
        "\n",
        "El hecho de que el Descenso de Gradiente pueda converger a diferentes soluciones basadas en el parámetro $\\gamma$ es una razón por la que este método, fundamental para el proceso de **retropropagación** en el entrenamiento de redes neuronales, es un campo activo de investigación y optimización."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1fcb8d-4751-4c17-a35d-4ccc6a870882",
      "metadata": {
        "id": "4b1fcb8d-4751-4c17-a35d-4ccc6a870882"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------\n",
        "2b iv. Aplica nuevamente el método sobre f(x)con x0= 3, γ= 0.1,tol=1e-12, maxit=1e5. Interpreta el resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b354a999-e385-4e78-9eaa-a9dd60e3ff4d",
      "metadata": {
        "id": "b354a999-e385-4e78-9eaa-a9dd60e3ff4d",
        "outputId": "e0ba1abc-c92b-4c74-c03c-59e89b7ac537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Detectado problema de 1 variable(s).\n",
            "--> Convergencia en iteración 3\n",
            "Mínimo en x=-87049978838294.5938, f(x)=172264558026542218147640427349032752992274081810034458624.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x0 = np.array([3.0])  # Valor inicial x0 ∈ R^n (n=1)\n",
        "gamma = 0.1         # Ratio de aprendizaje (γ)\n",
        "tol = 1e-12           # Tolerancia (tol)\n",
        "maxit = 1e5\n",
        "x_res, y_res = resolver_optimizacion(f1,x0, gamma, tol, maxit)\n",
        "print(f\"Mínimo en x={x_res[0]:.4f}, f(x)={y_res:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f248dbf-03ab-4d27-ba7e-2f209ff3eba9",
      "metadata": {
        "id": "5f248dbf-03ab-4d27-ba7e-2f209ff3eba9"
      },
      "source": [
        "El hecho de que el algoritmo produzca los errores `RuntimeWarning: overflow encountered in power` y `RuntimeWarning: invalid value encountered in subtract` al usar un ratio de aprendizaje ($\\gamma$) de $0.1$ se debe a la **divergencia explosiva** del método de Descenso de Gradiente.\n",
        "\n",
        "El **ratio de aprendizaje ($\\gamma$)** es un parámetro que modula la magnitud del paso que se toma en cada iteración en la dirección opuesta al gradiente ($\\nabla f$).\n",
        "\n",
        "1.  **Divergencia por Paso Excesivo:** Al establecer $\\gamma$ en $0.1$ (un valor cien veces mayor que el inicial $0.001$), los pasos tomados son demasiado grandes. Si el ratio de aprendizaje es muy grande, el Descenso de Gradiente puede **saltarse el mínimo local** o, peor aún, **divergir**, terminando en regiones con una desviación mucho mayor que la anterior.\n",
        "\n",
        "2.  **Cálculo Inicial de la Divergencia:**\n",
        "    *   La función que se minimiza tiene un gradiente $\\nabla f(x) = 12x^3 + 12x^2 - 24x$.\n",
        "    *   Al comenzar en $x_0 = 3.0$, el gradiente inicial es $\\nabla f(3) = 360$.\n",
        "    *   El primer paso es $x_1 = x_0 - \\gamma \\cdot \\nabla f(x_0) = 3.0 - 0.1 \\cdot 360 = -33$.\n",
        "\n",
        "*   Con $\\gamma=0.001$, el algoritmo convergió a un mínimo local ($x \\approx 1.0$).\n",
        "*   Con $\\gamma=0.01$, el algoritmo también logró converger a otro mínimo local ($x = -2$).\n",
        "\n",
        "La lección es que, si bien un ratio de aprendizaje mayor (como $0.01$) puede permitir que el algoritmo explore diferentes regiones del espacio de la función de coste (llegando a $x=-2$), un ratio demasiado alto (como $0.1$) resulta en inestabilidad y divergencia inmediata, lo cual es fatal para el proceso de optimización. Este es un ejemplo clave de por qué el **ajuste fino del ratio de aprendizaje ($\\gamma$)** es una tarea crucial en el entrenamiento de modelos de Inteligencia Artificial (como en el proceso de retropropagación), donde el cálculo es una herramienta esencial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88aa9623-ab6f-4ffa-83d5-d5d21f4d8bda",
      "metadata": {
        "id": "88aa9623-ab6f-4ffa-83d5-d5d21f4d8bda"
      },
      "source": [
        "------------------------------------------------------------------------------------------------\n",
        "2bv. Finalmente, aplica el método sobre f(x)con x0= 0, γ= 0.001,tol=1e-12, maxit=1e5. Interpreta el resultado y compáralo con el estudio anaĺıtico de f. ¿Se trata de un resultado deseable? ¿Por qué? ¿A qué se debe este fenómeno?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([0.0])  # Valor inicial x0 ∈ R^n (n=1)\n",
        "gamma = 0.001         # Ratio de aprendizaje (γ)\n",
        "tol = 1e-12           # Tolerancia (tol)\n",
        "maxit = 1e5\n",
        "x_res, y_res = resolver_optimizacion(f1,x0, gamma, tol, maxit)\n",
        "print(f\"Mínimo en x={x_res[0]:.4f}, f(x)={y_res:.4f}\\n\")"
      ],
      "metadata": {
        "id": "Qz7UWiabOhj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e82a266-3d32-47fd-9357-966198778dec"
      },
      "id": "Qz7UWiabOhj8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Detectado problema de 1 variable(s).\n",
            "--> Convergencia en iteración 1394\n",
            "Mínimo en x=-2.0000, f(x)=-25.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4d4736-496e-4c7c-8725-a4d07b0befbb",
      "metadata": {
        "id": "4c4d4736-496e-4c7c-8725-a4d07b0befbb"
      },
      "source": [
        "El algoritmo ha encontrado un **punto crítico** de la función en la primera iteración ($k=0$).\n",
        "\n",
        "El resultado que obtuvo, **`Convergencia alcanzada en la iteración 0 con ||∇f(x)||2 = 0.000000000000`**, indica que el algoritmo de Descenso de Gradiente terminó de inmediato porque el punto de inicio, $x_0=0$, ya cumple con el criterio de tolerancia establecido ($\\text{tol} = 1\\text{e-}12$).\n",
        "\n",
        "Cuando se aplica el Descenso de Gradiente a $x_0=0$:\n",
        "\n",
        "*   **Punto Crítico:** La función objetivo es $f(x) = 3x^4 + 4x^3 - 12x^2 + 7$.\n",
        "*   **Gradiente en $x_0$:** El algoritmo comienza calculando el gradiente ($\\nabla f(x)$), que es la derivada $f'(x) = 12x^3 + 12x^2 - 24x$.\n",
        "*   Al evaluar el gradiente en $x=0$:\n",
        "    $$f'(0) = 12(0)^3 + 12(0)^2 - 24(0) = 0$$\n",
        "*   **Convergencia Inmediata:** La norma del gradiente ($\\|\\nabla f(x)\\|_2$) en $x=0$ es cero, que es mucho menor que el valor de tolerancia ($\\text{tol} = 1\\text{e-}12$). Por lo tanto, el código Python ejecuta la instrucción `break` en la primera iteración ($k=0$), reportando que la convergencia ha sido alcanzada.\n",
        "\n",
        "### ¿Es un Mínimo Local?\n",
        "\n",
        "El algoritmo ha encontrado un **punto crítico**, donde la derivada es cero. Sin embargo, un punto crítico puede ser un máximo local, un mínimo local, o un punto de silla (punto de inflexión horizontal).\n",
        "\n",
        "Para la función $f(x) = 3x^4 + 4x^3 - 12x^2 + 7$, los puntos críticos se obtienen al resolver $f'(x)=0$:\n",
        "$$12x(x^2 + x - 2) = 0 \\implies 12x(x - 1)(x + 2) = 0$$\n",
        "Los puntos críticos son $x=0$, $x=1$ y $x=-2$\n",
        "\n",
        "Para determinar si $x=0$ es un mínimo o un máximo, se puede utilizar el criterio de la **segunda derivada** (o derivada segunda, $f''(x)$), un concepto del cálculo infinitesimal.\n",
        "\n",
        "*   **Segunda Derivada:** $f''(x) = 36x^2 + 24x - 24$.\n",
        "*   **Evaluación en $x=0$:** $f''(0) = 36(0)^2 + 24(0) - 24 = -24$.\n",
        "\n",
        "Dado que $f''(0) < 0$, el punto crítico $x=0$ corresponde a un **máximo local** de la función.\n",
        "\n",
        "### Conclusión\n",
        "\n",
        "El algoritmo **terminó en la primera iteración** porque el punto inicial $x_0=0$ es un punto crítico donde el gradiente es cero, satisfaciendo inmediatamente la condición de convergencia. Sin embargo, analíticamente, este punto crítico es un **máximo local**, no un mínimo local.\n",
        "\n",
        "Esto es un ejemplo de cómo el **Descenso de Gradiente**, un método de optimización crucial para el entrenamiento de redes neuronales, converge a un punto crítico cercano a $x_0$, pero no garantiza que dicho punto sea un mínimo global (o siquiera un mínimo local). En este caso, el algoritmo se \"detuvo\" directamente en un máximo local."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b28ad8-5d06-48b9-be76-9c6227c85121",
      "metadata": {
        "id": "39b28ad8-5d06-48b9-be76-9c6227c85121"
      },
      "source": [
        "------------------------------------------------------------------------------------------------\n",
        "2c. Sea la función g: R2 →R dada por g(x,y)= x2+ y3+ 3xy+ 1.\n",
        "2ci. Apĺıquese el método sobre g(x,y)con x0= (−1,1), γ= 0.01,tol=1e-12, maxit=1e5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4769f63a-311b-46cc-a6c4-b08d7b49c665",
      "metadata": {
        "id": "4769f63a-311b-46cc-a6c4-b08d7b49c665"
      },
      "source": [
        "Requiere aplicar el método de Descenso de Gradiente (GD) a una función multivariable, $g: \\mathbb{R}^2 \\to \\mathbb{R}$, donde el punto inicial es un vector $x_0 = (-1, 1)$, lo cual es un punto en el espacio bidimensional y no un escalar.\n",
        "\n",
        "La función a minimizar es:\n",
        "$$g(x, y) = x^2 + y^3 + 3xy + 1$$\n",
        "\n",
        "### 1. Cálculo del Gradiente $\\nabla g(x, y)$\n",
        "\n",
        "Para aplicar el método de Descenso de Gradiente, es esencial calcular el gradiente de la función, que consiste en el vector de derivadas parciales:\n",
        "$$\\nabla g(x, y) = \\begin{pmatrix} \\frac{\\partial g}{\\partial x} \\\\ \\frac{\\partial g}{\\partial y} \\end{pmatrix}$$\n",
        "\n",
        "1.  **Derivada parcial respecto a $x$:**\n",
        "    $$\\frac{\\partial g}{\\partial x} = 2x + 3y$$\n",
        "2.  **Derivada parcial respecto a $y$:**\n",
        "    $$\\frac{\\partial g}{\\partial y} = 3y^2 + 3x$$\n",
        "\n",
        "Por lo tanto, la función que implementa el gradiente (`grad_g`) será:\n",
        "$$\\nabla g(x, y) = \\begin{pmatrix} 2x + 3y \\\\ 3y^2 + 3x \\end{pmatrix}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f2(x, y):\n",
        "    return x**2 + y**3 + 3*x*y + 1"
      ],
      "metadata": {
        "id": "rmL15pp6PLZb"
      },
      "id": "rmL15pp6PLZb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19633fd-76b0-45bd-be10-d994ee6bf40f",
      "metadata": {
        "id": "f19633fd-76b0-45bd-be10-d994ee6bf40f",
        "outputId": "c177ad18-2c3b-47ba-d82b-0955ebf4e2fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Detectado problema de 2 variable(s).\n",
            "--> Convergencia en iteración 26098\n",
            "Mínimo en x=[-2.25  1.5 ], f(x)=-0.6875\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x0 = np.array([-1.0, 1.0]) # Valor inicial x0 ∈ R^2\n",
        "gamma = 0.001              # Ratio de aprendizaje (γ)\n",
        "tol = 1e-12               # Tolerancia (tol)\n",
        "maxit = 1e5\n",
        "\n",
        "x_res, y_res = resolver_optimizacion(f2, x0, gamma, tol, maxit )\n",
        "print(f\"Mínimo en x={x_res}, f(x)={y_res:.4f}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eef391d2-dbbd-4f9a-aade-56593202f285",
      "metadata": {
        "id": "eef391d2-dbbd-4f9a-aade-56593202f285"
      },
      "source": [
        "### 3. Resultados de la Ejecución\n",
        "\n",
        "El método de Descenso de Gradiente ha convergido a la aproximación:\n",
        "$$\\mathbf{x}^* \\approx (-2.2500000000, 1.5000000000)$$\n",
        "\n",
        "### 4. Interpretación Analítica (Contraste)\n",
        "\n",
        "La convergencia se ha producido en el punto $(-2.25, 1.5)$. Analíticamente, los puntos críticos de $g(x, y)$ se encuentran donde $\\nabla g(x, y) = 0$:\n",
        "1.  $2x + 3y = 0$\n",
        "2.  $3y^2 + 3x = 0$\n",
        "\n",
        "Al resolver este sistema, encontramos dos puntos críticos:\n",
        "*   $(0, 0)$\n",
        "*   $(-2.25, 1.5)$\n",
        "\n",
        "El algoritmo ha convergido con éxito a uno de los dos puntos críticos analíticos de la función $g$. El Descenso de Gradiente, al partir de $x_0 = (-1, 1)$ con un ratio de aprendizaje $\\gamma = 0.01$, siguió la trayectoria que lo condujo a la cuenca de atracción del mínimo local $(-2.25, 1.5)$. La capacidad del método para trabajar en espacios de mayor dimensión (como $\\mathbb{R}^2$ en este caso, a diferencia de la función $f(x)$ anterior que era de $\\mathbb{R}$ a $\\mathbb{R}$) es esencial para la optimización en el *Cálculo Multivariable* y en aplicaciones de IA, como la retropropagación."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269a8c0c-dd80-4760-acc9-4a5a96292912",
      "metadata": {
        "id": "269a8c0c-dd80-4760-acc9-4a5a96292912"
      },
      "source": [
        "------------------------------------------------------------------------------------------------\n",
        "2cii ¿Qué ocurre si ahora partimos de x0= (0,0)? ¿Se obtiene un resultado deseable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b777d3-5253-41ed-b6df-7d7d7ccdb64f",
      "metadata": {
        "id": "b5b777d3-5253-41ed-b6df-7d7d7ccdb64f",
        "outputId": "39a848a3-cc62-409a-9536-1fc1e22fd54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Detectado problema de 2 variable(s).\n",
            "--> Convergencia en iteración 11153\n",
            "Mínimo en x=[ 2.60171334e+02 -1.03045417e+12], f(x)=-1094173123513588401999962528364363776.0000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x0 = np.array([0.0, 0.0]) # Valor inicial x0 ∈ R^2\n",
        "gamma = 0.001              # Ratio de aprendizaje (γ)\n",
        "tol = 1e-12               # Tolerancia (tol)\n",
        "maxit = 1e5\n",
        "x_res, y_res = resolver_optimizacion(f2, x0, gamma, tol, maxit )\n",
        "print(f\"Mínimo en x={x_res}, f(x)={y_res:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff3aa68-55b7-4cf6-8a82-70e268250d43",
      "metadata": {
        "id": "4ff3aa68-55b7-4cf6-8a82-70e268250d43"
      },
      "source": [
        "### Explicación de la Convergencia Inmediata\n",
        "\n",
        "1.  **Cálculo del Gradiente:** El método de Descenso de Gradiente se basa en moverse en la dirección opuesta al vector gradiente ($\\nabla g$) hasta que la magnitud de este vector es menor que la tolerancia ($\\text{tol}$).\n",
        "    El gradiente de $g(x, y)$ está definido por:\n",
        "    $$\\nabla g(x, y) = \\begin{pmatrix} 2x + 3y \\\\ 3y^2 + 3x \\end{pmatrix}$$\n",
        "2.  **Evaluación en el Punto Inicial ($x_0$):** Al evaluar el gradiente en $x_0 = (0, 0)$:\n",
        "\n",
        "    $$\\nabla g(0, 0) = \\begin{pmatrix} 2(0) + 3(0) \\\\ 3(0)^2 + 3(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n",
        "3.  **Divergencia del algoritmo:** La razón de la divergencia es la notauraleza de que (0,0) es un punto de silla y el término $y^3$, este punto de silla es inestable, es decir, si empezamos en (0,0) y el algoritmo da un pequeño paso en la dirección $y^3$ el término cúbico empeará a dominar y dado que la función no tiene un minimo global esta al moverse empezará a diverger.\n",
        "\n",
        "### Interpretación Analítica del Punto\n",
        "\n",
        "El gradiente, en el punto $(0, 0)$ es un **punto crítico** de la función $g(x, y)$\n",
        "\n",
        "Sin embargo, desde la perspectiva del **Cálculo multivariable**, el punto $(0, 0)$ no es un mínimo local, sino un **punto de silla**. Esto se determina analizando la matriz Hessiana de la función en ese punto.\n",
        "\n",
        "*   El cálculo del determinante de la matriz Hessiana de $g(x, y)$ evaluada en $(0, 0)$ es negativo ($\\det(H(0, 0)) = -9$).\n",
        "*   Cuando el determinante de la matriz Hessiana es negativo, el punto crítico se clasifica como un **punto de silla** (ni máximo ni mínimo local).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------\n",
        "\n",
        "### **3.** Redacte una breve reseña en la que exponga los beneficios obtenidos a partir de la experiencia de trabajo colaborativo en grupo, destacando los aspectos positivos que contribuyeron al desarrollo personal y colectivo. Se evaluará, como competencia transversal, la disposición del estudiante para colaborar con sus compañeros, compartir ideas, asumir responsabilidades dentro del grupo y contribuir al logro de los objetivos comunes. Se valorará especialmente la comunicación, el apoyo mutuo y la capacidad de integrar diferentes puntos de vista."
      ],
      "metadata": {
        "id": "pvzeBgHOQG3F"
      },
      "id": "pvzeBgHOQG3F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La elaboración de este trabajo para la asignatura de Matemáticas para la IA ha representado un reto que ha ido más allá de lo técnico, permitiéndonos desarrollar competencias transversales esenciales en el ámbito profesional y académico. A pesar de las dificultades lógicas para coordinar agendas en un entorno de máster, la experiencia ha sido sumamente enriquecedora gracias a una metodología de trabajo basada en el compromiso y la transparencia.\n",
        "\n",
        "Los aspectos fundamentales que permitieron el éxito del grupo fueron:\n",
        "\n",
        "- **Comunicación Eficiente y Asíncrona**: Entendiendo que no siempre podíamos reunirnos de forma simultánea, establecimos canales de comunicación constantes que permitieron un flujo de información ininterrumpido. El apoyo mutuo fue clave para resolver dudas complejas, especialmente en la interpretación de los resultados del descenso de gradiente y la implementación de la eliminación gaussiana.\n",
        "\n",
        "- **División de Responsabilidades y Feedback de Unificación**: El trabajo no se planteó como una simple suma de partes aisladas. Cada bloque fue compartido con el grupo conforme se avanzaba, permitiendo que el resto aportara ideas o correcciones. Este proceso de retroalimentación fue lo que realmente permitió una unificación orgánica del trabajo, asegurando que todos comprendiéramos el razonamiento matemático detrás de cada ejercicio.\n",
        "\n",
        "- **Revisión Individual**: Antes de dar por finalizada cualquier sección, cada integrante realizó una revisión del trabajo. Este paso de \"doble validación\" fue fundamental para identificar posibles errores en el código o en las fórmulas de los determinantes, garantizando la precisión técnica del entregable.\n",
        "\n",
        "- **Sesiones de Consenso Final**: Una vez completadas las revisiones individuales, organizamos reuniones estratégicas para poner en común los hallazgos. En estas sesiones resaltamos los puntos más críticos como la sensibilidad de los algoritmos al ratio de aprendizaje y debatimos hasta alcanzar un consenso total sobre las interpretaciones finales. Esta capacidad de integrar diferentes puntos de vista fortaleció el resultado colectivo.\n",
        "\n",
        "En conclusión, esta experiencia ha contribuido significativamente al desarrollo personal de cada uno de nosotros. Hemos aprendido que la calidad de un trabajo no depende solo del contenido colocado, sino de la capacidad del equipo para colaborar, asumir responsabilidades compartidas y mantener una comunicación constructiva orientada a un objetivo común."
      ],
      "metadata": {
        "id": "MBsCsHCNQbaS"
      },
      "id": "MBsCsHCNQbaS"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Q9l4t8bJY0I"
      },
      "id": "2Q9l4t8bJY0I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Master",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}